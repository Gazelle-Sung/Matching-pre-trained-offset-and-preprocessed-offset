{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# *** Select a preprocessing method\n",
    "def sel_preprocessing(_input=\"stanza\"):\n",
    "    # 1. Stanza\n",
    "    if _input.lower() == \"stanza\":\n",
    "        import stanza\n",
    "        return stanza.Pipeline('en')\n",
    "    \n",
    "    elif _input.lower() == \"spacy\":\n",
    "        # 2. Spacy\n",
    "        import spacy\n",
    "        return spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "        \n",
    "# *** Select a pre-trained Word Embeddings\n",
    "def sel_pretrained(_input=\"bert\"):\n",
    "    # 1. BERT\n",
    "    if _input.lower() == \"bert\":\n",
    "        from transformers import BertConfig, BertModel, BertTokenizerFast, BertForSequenceClassification, AdamW, BertPreTrainedModel\n",
    "        PRETRAINED_BERT = \"bert-base-uncased\"\n",
    "        return BertModel.from_pretrained(PRETRAINED_BERT), BertTokenizerFast.from_pretrained(PRETRAINED_BERT)\n",
    "\n",
    "    # 2. Elmo\n",
    "    elif _input.lower() == \"elmo\":\n",
    "        from allennlp.commands.elmo import ElmoEmbedder\n",
    "        return None, ElmoEmbedder(cuda_device=0)\n",
    "\n",
    "    # 3. Glove\n",
    "    elif _input.lower() == \"glove42b\":\n",
    "        glove_PATH = \"./glove/glove.42B.300d.pkl\"\n",
    "        with open(glove_PATH, \"rb\") as file:\n",
    "            glove = pickle.load(file)\n",
    "        return None, glove\n",
    "    \n",
    "    elif _input.lower() == \"glove840b\":\n",
    "        glove_PATH = \"./glove/glove.840B.300d.pkl\"\n",
    "        with open(glove_PATH, \"rb\") as file:\n",
    "            glove = pickle.load(file)\n",
    "        return None, glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clause_feature:\n",
    "    def __init__(self, tk_idx, tk, offset, pos, ner, dep, bert_offset):\n",
    "        #self.c_idx = c_idx\n",
    "        self.tk_idx = tk_idx\n",
    "        self.tk = tk\n",
    "        self.offset = offset\n",
    "        self.pos = pos\n",
    "        self.ner = ner\n",
    "        self.dep = dep\n",
    "        self.bert_offset = bert_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_processing(_datasample, nlp, _pipeline='stanza'):\n",
    "    cur_clause_feature = []\n",
    "    # Spacy\n",
    "    if _pipeline == \"spacy\":\n",
    "        doc = nlp(_datasample)\n",
    "        for tk_idx, tk in enumerate(doc):\n",
    "            cur_tk = tk.text\n",
    "            cur_id = tk_idx+1\n",
    "            cur_head = tk.head.i\n",
    "            \n",
    "            cur_tk_start_offset = tk.idx\n",
    "            cur_tk_end_offset = tk.idx + len(tk)\n",
    "            \n",
    "            cur_pos = tk.pos_\n",
    "            cur_ner = tk.ent_type_\n",
    "            cur_dep = tk.dep_\n",
    "            gov_idx = (cur_id, cur_dep, cur_head)\n",
    "            \n",
    "            cur_dep_triple = (cur_id, cur_dep, cur_head)\n",
    "                    \n",
    "            cur_clause_feature.append(Clause_feature(cur_id, cur_tk, (cur_tk_start_offset, cur_tk_end_offset), cur_pos, cur_ner, cur_dep_triple, ''))\n",
    "    # Stanza\n",
    "    elif _pipeline == \"stanza\":\n",
    "        doc = nlp(_datasample)\n",
    "        for sen in doc.sentences:\n",
    "            for tk in sen.tokens:\n",
    "                tk_infor_dict = tk.to_dict()[0]\n",
    "                cur_tk = tk_infor_dict[\"text\"]        \n",
    "                cur_id = tk_infor_dict['id']\n",
    "                cur_head = tk_infor_dict['head']\n",
    "\n",
    "                offsets = tk_infor_dict['misc'].split(\"|\")\n",
    "                cur_tk_start_offset = int(offsets[0].split(\"=\")[1])\n",
    "                cur_tk_end_offset = int(offsets[1].split(\"=\")[1])\n",
    "\n",
    "                cur_pos = tk_infor_dict[\"xpos\"]\n",
    "                cur_ner = tk_infor_dict[\"ner\"]\n",
    "\n",
    "                cur_dep = tk_infor_dict[\"deprel\"]\n",
    "                gov_idx = tk_infor_dict[\"head\"]\n",
    "\n",
    "                cur_dep_triple = (cur_id, cur_dep, cur_head)\n",
    "\n",
    "                cur_clause_feature.append(Clause_feature(cur_id, cur_tk, (cur_tk_start_offset, cur_tk_end_offset), cur_pos, cur_ner, cur_dep_triple, ''))\n",
    "    return cur_clause_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pipeline = \"spacy\"\n",
    "emb = \"bert\"\n",
    "\n",
    "nlp = sel_preprocessing(_pipeline)\n",
    "model, embedder = sel_pretrained(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"Officials are set to announce details of B.C.'s latest restart plan on Tuesday as daily case counts continue to trend downward and hours after the last round of \"circuit breaker\" restrictions expired.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tOfficials\t(0, 9)\tNOUN\t(1, 'nsubjpass', 2)\t\n",
      "2\tare\t(10, 13)\tVERB\t(2, 'auxpass', 2)\t\n",
      "3\tset\t(14, 17)\tVERB\t(3, 'ROOT', 2)\t\n",
      "4\tto\t(18, 20)\tPART\t(4, 'aux', 4)\t\n",
      "5\tannounce\t(21, 29)\tVERB\t(5, 'xcomp', 2)\t\n",
      "6\tdetails\t(30, 37)\tNOUN\t(6, 'dobj', 4)\t\n",
      "7\tof\t(38, 40)\tADP\t(7, 'prep', 5)\t\n",
      "8\tB.C.\t(41, 45)\tPROPN\t(8, 'poss', 11)\t\n",
      "9\t's\t(45, 47)\tPART\t(9, 'case', 7)\t\n",
      "10\tlatest\t(48, 54)\tADJ\t(10, 'amod', 11)\t\n",
      "11\trestart\t(55, 62)\tNOUN\t(11, 'compound', 11)\t\n",
      "12\tplan\t(63, 67)\tNOUN\t(12, 'pobj', 6)\t\n",
      "13\ton\t(68, 70)\tADP\t(13, 'prep', 4)\t\n",
      "14\tTuesday\t(71, 78)\tPROPN\t(14, 'pobj', 12)\t\n",
      "15\tas\t(79, 81)\tADP\t(15, 'mark', 18)\t\n",
      "16\tdaily\t(82, 87)\tADJ\t(16, 'amod', 16)\t\n",
      "17\tcase\t(88, 92)\tNOUN\t(17, 'compound', 17)\t\n",
      "18\tcounts\t(93, 99)\tNOUN\t(18, 'nsubj', 18)\t\n",
      "19\tcontinue\t(100, 108)\tVERB\t(19, 'advcl', 4)\t\n",
      "20\tto\t(109, 111)\tPART\t(20, 'aux', 20)\t\n",
      "21\ttrend\t(112, 117)\tVERB\t(21, 'xcomp', 18)\t\n",
      "22\tdownward\t(118, 126)\tADV\t(22, 'advmod', 20)\t\n",
      "23\tand\t(127, 130)\tCCONJ\t(23, 'cc', 21)\t\n",
      "24\thours\t(131, 136)\tNOUN\t(24, 'conj', 21)\t\n",
      "25\tafter\t(137, 142)\tADP\t(25, 'mark', 34)\t\n",
      "26\tthe\t(143, 146)\tDET\t(26, 'det', 27)\t\n",
      "27\tlast\t(147, 151)\tADJ\t(27, 'amod', 27)\t\n",
      "28\tround\t(152, 157)\tNOUN\t(28, 'nsubj', 34)\t\n",
      "29\tof\t(158, 160)\tADP\t(29, 'prep', 27)\t\n",
      "30\t\"\t(161, 162)\tPUNCT\t(30, 'punct', 31)\t\n",
      "31\tcircuit\t(162, 169)\tNOUN\t(31, 'nmod', 31)\t\n",
      "32\tbreaker\t(170, 177)\tNOUN\t(32, 'nmod', 33)\t\n",
      "33\t\"\t(177, 178)\tPUNCT\t(33, 'punct', 33)\t\n",
      "34\trestrictions\t(179, 191)\tNOUN\t(34, 'pobj', 28)\t\n",
      "35\texpired\t(192, 199)\tVERB\t(35, 'advcl', 20)\t\n",
      "36\t.\t(199, 200)\tPUNCT\t(36, 'punct', 2)\t\n"
     ]
    }
   ],
   "source": [
    "my_result = tokenization_processing(sample_text, nlp, _pipeline)\n",
    "for x in my_result:\n",
    "    print(\"{}\\t{}\\t{}\\t{}\\t{}\\t{}\".format(x.tk_idx, x.tk, x.offset, x.pos, x.dep, x.bert_offset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# version 1 - Bert -> input_ids, attention_mask, token_types_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding2preprocessed(datasample, nlp, _pipeline, embedder, emb=\"bert\"):\n",
    "    preprocessed_result = tokenization_processing(datasample, nlp, _pipeline)\n",
    "    \n",
    "    if emb == \"bert\":\n",
    "        # bert tokenized result\n",
    "        tokenized_result = embedder(datasample, return_offsets_mapping=True)\n",
    "\n",
    "        bert2preprocessed_offset = []\n",
    "        for p_idx, p_result in enumerate(preprocessed_result):\n",
    "            p_offset = p_result.offset\n",
    "            p_start_offset = p_offset[0]\n",
    "            p_end_offset = p_offset[1]\n",
    "\n",
    "            cur_offset_list = []\n",
    "            for b_idx, b_offset in enumerate(tokenized_result['offset_mapping'][1:-1]):\n",
    "                b_start_offset = b_offset[0]\n",
    "                b_end_offset = b_offset[1]\n",
    "\n",
    "                # offset of the bert tokenized token is in the preprocessed offset\n",
    "                if b_start_offset >= p_start_offset and b_end_offset <= p_end_offset:\n",
    "                    if len(cur_offset_list) == 0:\n",
    "                        cur_offset_list = [b_idx+1]\n",
    "                    else:\n",
    "                        cur_offset_list.append(b_idx+1)\n",
    "\n",
    "\n",
    "            p_result.bert_offset = cur_offset_list\n",
    "\n",
    "        return {'input_ids':tokenized_result['input_ids'],\n",
    "                 'attention_mask':tokenized_result['attention_mask'],\n",
    "                 'token_type_ids':tokenized_result['token_type_ids'],\n",
    "                 'preprocessed_offset_match':[x.bert_offset for x in preprocessed_result],\n",
    "                 'preprocessed_dep': [x.dep for x in preprocessed_result]}\n",
    "    \n",
    "    elif emb ==\"elmo\":\n",
    "        elmo_embedding = embedder.embed_sentence([x.tk for x in preprocessed_result])[0]\n",
    "        elmo_embedding = elmo_embedding.cpu().clone().detach()\n",
    "        \n",
    "        return {'elmo_emb':elmo_embedding,\n",
    "                 'preprocessed_offset_match':[[x.tk_idx] for x in preprocessed_result],\n",
    "                 'preprocessed_dep': [x.dep for x in preprocessed_result]}\n",
    "    \n",
    "    elif emb == 'glove':\n",
    "        glove_emb = []\n",
    "        \n",
    "        for x in preprocessed_result:\n",
    "            cur_tk = x.tk.lower()\n",
    "            if cur_tk in embedder:\n",
    "                cur_emb = embedder[cur_tk]\n",
    "            else:\n",
    "                cur_emb = np.zeros(300)\n",
    "            glove_emb.append(cur_emb)\n",
    "        glove_emb = torch.tensor(glove_emb)\n",
    "        \n",
    "        return {'glove_emb':glove_emb,\n",
    "                 'preprocessed_offset_match':[[x.tk_idx] for x in preprocessed_result],\n",
    "                 'preprocessed_dep': [x.dep for x in preprocessed_result]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 4584, 2024, 2275, 2000, 14970, 4751, 1997, 1038, 1012, 1039, 1012, 1005, 1055, 6745, 23818, 2933, 2006, 9857, 2004, 3679, 2553, 9294, 3613, 2000, 9874, 14047, 1998, 2847, 2044, 1996, 2197, 2461, 1997, 1000, 4984, 24733, 1000, 9259, 13735, 1012, 102]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1], [2], [3], [4], [5], [6], [7], [8, 9, 10, 11], [12, 13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40]]\n",
      "[(1, 'nsubjpass', 2), (2, 'auxpass', 2), (3, 'ROOT', 2), (4, 'aux', 4), (5, 'xcomp', 2), (6, 'dobj', 4), (7, 'prep', 5), (8, 'poss', 11), (9, 'case', 7), (10, 'amod', 11), (11, 'compound', 11), (12, 'pobj', 6), (13, 'prep', 4), (14, 'pobj', 12), (15, 'mark', 18), (16, 'amod', 16), (17, 'compound', 17), (18, 'nsubj', 18), (19, 'advcl', 4), (20, 'aux', 20), (21, 'xcomp', 18), (22, 'advmod', 20), (23, 'cc', 21), (24, 'conj', 21), (25, 'mark', 34), (26, 'det', 27), (27, 'amod', 27), (28, 'nsubj', 34), (29, 'prep', 27), (30, 'punct', 31), (31, 'nmod', 31), (32, 'nmod', 33), (33, 'punct', 33), (34, 'pobj', 28), (35, 'advcl', 20), (36, 'punct', 2)]\n"
     ]
    }
   ],
   "source": [
    "_pipeline = \"spacy\"\n",
    "emb = \"bert\"\n",
    "\n",
    "nlp = sel_preprocessing(_pipeline)\n",
    "embedder = sel_pretrained(emb)\n",
    "\n",
    "preprocessed_bert_input_result = embedding2preprocessed(sample_text, nlp, _pipeline, embedder, emb=emb)\n",
    "print(preprocessed_bert_input_result['input_ids'])\n",
    "print(preprocessed_bert_input_result['attention_mask'])\n",
    "print(preprocessed_bert_input_result['token_type_ids'])\n",
    "print(preprocessed_bert_input_result['preprocessed_offset_match'])\n",
    "print(preprocessed_bert_input_result['preprocessed_dep'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# version 3: frozen embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding2preprocessed(datasample, nlp, _pipeline, embedder, model, emb=\"bert\"):\n",
    "    # preprocessed result\n",
    "    preprocessed_result = tokenization_processing(datasample, nlp, _pipeline)\n",
    "    \n",
    "    if emb == \"bert\":\n",
    "        # bert tokenized result\n",
    "        tokenized_result = embedder(datasample, return_offsets_mapping=True, return_tensors='pt')\n",
    "        bert_embedding = model(input_ids=tokenized_result['input_ids'],\n",
    "                              attention_mask=tokenized_result['attention_mask'],\n",
    "                              token_type_ids=tokenized_result['token_type_ids'])[0]\n",
    "        \n",
    "        bert_embedding = torch.squeeze(bert_embedding, 0)\n",
    "\n",
    "        bert2preprocessed_offset = []\n",
    "        for p_idx, p_result in enumerate(preprocessed_result):\n",
    "            p_offset = p_result.offset\n",
    "            p_start_offset = p_offset[0]\n",
    "            p_end_offset = p_offset[1]\n",
    "\n",
    "            cur_offset_list = []\n",
    "            for b_idx, b_offset in enumerate(tokenized_result['offset_mapping'][1:-1]):\n",
    "                b_start_offset = b_offset[0]\n",
    "                b_end_offset = b_offset[1]\n",
    "\n",
    "                # offset of the bert tokenized token is in the preprocessed offset\n",
    "                if b_start_offset >= p_start_offset and b_end_offset <= p_end_offset:\n",
    "                    if len(cur_offset_list) == 0:\n",
    "                        cur_offset_list = [b_idx+1]\n",
    "                    else:\n",
    "                        cur_offset_list.append(b_idx+1)\n",
    "\n",
    "            p_result.bert_offset = cur_offset_list\n",
    "\n",
    "        return {'bert_emb':bert_embedding,\n",
    "                 'preprocessed_offset_match':[x.bert_offset for x in preprocessed_result],\n",
    "                 'preprocessed_dep': [x.dep for x in preprocessed_result]}\n",
    "    \n",
    "    elif emb ==\"elmo\":\n",
    "        elmo_embedding = embedder.embed_sentence([x.tk for x in preprocessed_result])[0]\n",
    "        elmo_embedding = torch.tensor(elmo_embedding)\n",
    "        \n",
    "        return {'elmo_emb':elmo_embedding,\n",
    "                 'preprocessed_offset_match':[[x.tk_idx] for x in preprocessed_result],\n",
    "                 'preprocessed_dep': [x.dep for x in preprocessed_result]}\n",
    "    \n",
    "    elif 'glove' in emb:\n",
    "        glove_emb = []\n",
    "        \n",
    "        for x in preprocessed_result:\n",
    "            cur_tk = x.tk.lower()\n",
    "            if cur_tk in embedder:\n",
    "                cur_emb = embedder[cur_tk]\n",
    "            else:\n",
    "                cur_emb = np.zeros(300)\n",
    "            glove_emb.append(cur_emb)\n",
    "        glove_emb = torch.tensor(glove_emb)\n",
    "        \n",
    "        return {'glove_emb':glove_emb,\n",
    "                 'preprocessed_offset_match':[[x.tk_idx] for x in preprocessed_result],\n",
    "                 'preprocessed_dep': [x.dep for x in preprocessed_result]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"Officials are set to announce details of B.C.'s latest restart plan on Tuesday as daily case counts continue to trend downward and hours after the last round of \"circuit breaker\" restrictions expired.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0033, -0.2353,  0.7526,  ...,  0.0147,  0.5923,  0.0296],\n",
      "        [ 0.3014, -0.0032, -0.1142,  ..., -0.2120,  0.8723,  0.0863],\n",
      "        [ 0.6135, -0.1681,  0.3704,  ..., -0.2918,  0.3635, -0.3355],\n",
      "        ...,\n",
      "        [ 0.2405, -0.1017,  0.4480,  ...,  0.2065, -0.1057,  0.0970],\n",
      "        [ 0.4867,  0.1187,  0.1823,  ...,  0.2124, -0.1039, -0.2119],\n",
      "        [ 0.4026,  0.3242,  0.3778,  ...,  0.2004, -0.3061, -0.2883]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "[[1], [2], [3], [4], [5], [6], [7], [8, 9, 10, 11], [12, 13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36], [37], [38], [39], [40]]\n",
      "[(1, 'nsubjpass', 2), (2, 'auxpass', 2), (3, 'ROOT', 2), (4, 'aux', 4), (5, 'xcomp', 2), (6, 'dobj', 4), (7, 'prep', 5), (8, 'poss', 11), (9, 'case', 7), (10, 'amod', 11), (11, 'compound', 11), (12, 'pobj', 6), (13, 'prep', 4), (14, 'pobj', 12), (15, 'mark', 18), (16, 'amod', 16), (17, 'compound', 17), (18, 'nsubj', 18), (19, 'advcl', 4), (20, 'aux', 20), (21, 'xcomp', 18), (22, 'advmod', 20), (23, 'cc', 21), (24, 'conj', 21), (25, 'mark', 34), (26, 'det', 27), (27, 'amod', 27), (28, 'nsubj', 34), (29, 'prep', 27), (30, 'punct', 31), (31, 'nmod', 31), (32, 'nmod', 33), (33, 'punct', 33), (34, 'pobj', 28), (35, 'advcl', 20), (36, 'punct', 2)]\n"
     ]
    }
   ],
   "source": [
    "_pipeline = \"spacy\"\n",
    "emb = \"bert\"\n",
    "\n",
    "nlp = sel_preprocessing(_pipeline)\n",
    "model, embedder = sel_pretrained(emb)\n",
    "\n",
    "preprocessed_bert_input_result = embedding2preprocessed(sample_text, nlp, _pipeline, embedder, model, emb)\n",
    "print(preprocessed_bert_input_result['bert_emb'])\n",
    "print(preprocessed_bert_input_result['preprocessed_offset_match'])\n",
    "print(preprocessed_bert_input_result['preprocessed_dep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3627,  0.5986, -0.5457,  ...,  0.2147,  0.2961, -0.2138],\n",
      "        [-0.0312,  0.0804, -0.2824,  ...,  0.0382,  0.4789,  0.0865],\n",
      "        [-0.0687, -0.0184,  0.8037,  ...,  0.3777, -0.1891,  0.1027],\n",
      "        ...,\n",
      "        [-0.3913,  0.2549, -0.0263,  ...,  0.0189,  0.5630,  1.2168],\n",
      "        [-0.5887,  1.0959, -0.0608,  ..., -0.5536,  0.3853,  0.7061],\n",
      "        [-0.8872, -0.2004, -1.0601,  ..., -0.2655,  0.2115,  0.1977]])\n",
      "[[1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36]]\n",
      "[(1, 'nsubjpass', 2), (2, 'auxpass', 2), (3, 'ROOT', 2), (4, 'aux', 4), (5, 'xcomp', 2), (6, 'dobj', 4), (7, 'prep', 5), (8, 'poss', 11), (9, 'case', 7), (10, 'amod', 11), (11, 'compound', 11), (12, 'pobj', 6), (13, 'prep', 4), (14, 'pobj', 12), (15, 'mark', 18), (16, 'amod', 16), (17, 'compound', 17), (18, 'nsubj', 18), (19, 'advcl', 4), (20, 'aux', 20), (21, 'xcomp', 18), (22, 'advmod', 20), (23, 'cc', 21), (24, 'conj', 21), (25, 'mark', 34), (26, 'det', 27), (27, 'amod', 27), (28, 'nsubj', 34), (29, 'prep', 27), (30, 'punct', 31), (31, 'nmod', 31), (32, 'nmod', 33), (33, 'punct', 33), (34, 'pobj', 28), (35, 'advcl', 20), (36, 'punct', 2)]\n"
     ]
    }
   ],
   "source": [
    "_pipeline = \"spacy\"\n",
    "emb = \"elmo\"\n",
    "\n",
    "nlp = sel_preprocessing(_pipeline)\n",
    "model, embedder = sel_pretrained(emb)\n",
    "\n",
    "preprocessed_bert_input_result = embedding2preprocessed(sample_text, nlp, _pipeline, embedder, model, emb)\n",
    "print(preprocessed_bert_input_result['elmo_emb'])\n",
    "print(preprocessed_bert_input_result['preprocessed_offset_match'])\n",
    "print(preprocessed_bert_input_result['preprocessed_dep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2091,  0.1485,  0.0992,  ...,  0.2235, -0.0291, -0.1808],\n",
      "        [-0.3214,  0.1153,  0.0094,  ..., -0.2877,  0.2200,  0.1956],\n",
      "        [-0.3182,  0.1454, -0.1936,  ...,  0.1443, -0.0095, -0.1202],\n",
      "        ...,\n",
      "        [ 0.0442,  0.1085,  0.2111,  ..., -0.5376, -0.5948,  0.0439],\n",
      "        [-0.3451, -0.2852,  0.0509,  ..., -0.3698, -0.5903,  0.0912],\n",
      "        [ 0.1088,  0.0022,  0.2221,  ..., -0.2970,  0.1594, -0.1490]],\n",
      "       dtype=torch.float64)\n",
      "[[1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36]]\n",
      "[(1, 'nsubjpass', 2), (2, 'auxpass', 2), (3, 'ROOT', 2), (4, 'aux', 4), (5, 'xcomp', 2), (6, 'dobj', 4), (7, 'prep', 5), (8, 'poss', 11), (9, 'case', 7), (10, 'amod', 11), (11, 'compound', 11), (12, 'pobj', 6), (13, 'prep', 4), (14, 'pobj', 12), (15, 'mark', 18), (16, 'amod', 16), (17, 'compound', 17), (18, 'nsubj', 18), (19, 'advcl', 4), (20, 'aux', 20), (21, 'xcomp', 18), (22, 'advmod', 20), (23, 'cc', 21), (24, 'conj', 21), (25, 'mark', 34), (26, 'det', 27), (27, 'amod', 27), (28, 'nsubj', 34), (29, 'prep', 27), (30, 'punct', 31), (31, 'nmod', 31), (32, 'nmod', 33), (33, 'punct', 33), (34, 'pobj', 28), (35, 'advcl', 20), (36, 'punct', 2)]\n"
     ]
    }
   ],
   "source": [
    "_pipeline = \"spacy\"\n",
    "emb = \"glove42b\"\n",
    "\n",
    "nlp = sel_preprocessing(_pipeline)\n",
    "model, embedder = sel_pretrained(emb)\n",
    "\n",
    "preprocessed_bert_input_result = embedding2preprocessed(sample_text, nlp, _pipeline, embedder, model, emb)\n",
    "print(preprocessed_bert_input_result['glove_emb'])\n",
    "print(preprocessed_bert_input_result['preprocessed_offset_match'])\n",
    "print(preprocessed_bert_input_result['preprocessed_dep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2082, -0.2510,  0.5810,  ...,  0.1680,  0.2455, -0.0169],\n",
      "        [-0.1986, -0.0628, -0.3661,  ..., -0.5845,  0.2788, -0.2621],\n",
      "        [ 0.3637,  0.0710, -0.0676,  ..., -0.0436, -0.0298, -0.2876],\n",
      "        ...,\n",
      "        [ 0.2196, -0.1482, -0.3005,  ...,  0.1036,  0.5923,  0.4303],\n",
      "        [ 0.5271, -0.5270,  0.0362,  ...,  0.2272,  0.2620,  0.0738],\n",
      "        [ 0.0120,  0.2075, -0.1258,  ...,  0.1387, -0.3605, -0.0350]],\n",
      "       dtype=torch.float64)\n",
      "[[1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31], [32], [33], [34], [35], [36]]\n",
      "[(1, 'nsubjpass', 2), (2, 'auxpass', 2), (3, 'ROOT', 2), (4, 'aux', 4), (5, 'xcomp', 2), (6, 'dobj', 4), (7, 'prep', 5), (8, 'poss', 11), (9, 'case', 7), (10, 'amod', 11), (11, 'compound', 11), (12, 'pobj', 6), (13, 'prep', 4), (14, 'pobj', 12), (15, 'mark', 18), (16, 'amod', 16), (17, 'compound', 17), (18, 'nsubj', 18), (19, 'advcl', 4), (20, 'aux', 20), (21, 'xcomp', 18), (22, 'advmod', 20), (23, 'cc', 21), (24, 'conj', 21), (25, 'mark', 34), (26, 'det', 27), (27, 'amod', 27), (28, 'nsubj', 34), (29, 'prep', 27), (30, 'punct', 31), (31, 'nmod', 31), (32, 'nmod', 33), (33, 'punct', 33), (34, 'pobj', 28), (35, 'advcl', 20), (36, 'punct', 2)]\n"
     ]
    }
   ],
   "source": [
    "_pipeline = \"spacy\"\n",
    "emb = \"glove840b\"\n",
    "\n",
    "nlp = sel_preprocessing(_pipeline)\n",
    "model, embedder = sel_pretrained(emb)\n",
    "\n",
    "preprocessed_bert_input_result = embedding2preprocessed(sample_text, nlp, _pipeline, embedder, model, emb)\n",
    "print(preprocessed_bert_input_result['glove_emb'])\n",
    "print(preprocessed_bert_input_result['preprocessed_offset_match'])\n",
    "print(preprocessed_bert_input_result['preprocessed_dep'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
